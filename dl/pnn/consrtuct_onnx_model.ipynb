{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6c8217-47ac-4cd2-b13a-b6cf84a73184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx.helper import *\n",
    "import numpy as np\n",
    "from functools import reduce # Valid in Python 2.6+, required in Python 3\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a29276-6e05-48b1-8bb9-4c103a096305",
   "metadata": {},
   "source": [
    "## Make node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81008202-f896-4f6f-aea2-8fc88e163126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56172f15-0e9c-4b2f-bf42-221bfca09701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79e24fd-a820-4cfa-bb92-accee5bb68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    " m = nn.Conv2d(32, 32 , 3, stride=2,padding = 1, groups = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd00d42-18e6-4d67-a706-b330682f6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 32, 104, 104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73577e2d-335d-4fc9-9c81-9012cbe6f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "192ec00a-69a5-4ce4-8ca9-15c1692af1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(m,               # model being run\n",
    "                  data,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/Users/bob/code/CodeReading/Paddle-Lite/pnn/conv_depthwise.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['data'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b8fb4d-f928-46bf-ba78-a6406e86a97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 52, 52])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6a4c62-7b0d-4694-a30c-ac8685cfe721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2141,  0.3037, -0.0095, -0.2251, -0.2738,  0.3105,  0.0239, -0.2920,\n",
       "        -0.3256, -0.2038, -0.2915, -0.1269, -0.0550,  0.2707, -0.3201, -0.3210,\n",
       "        -0.3223, -0.0352, -0.2593, -0.0653, -0.1905, -0.3320, -0.2805, -0.2138,\n",
       "        -0.3333, -0.1204,  0.2564,  0.1934, -0.0482, -0.2368, -0.1808, -0.2981],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43662b1b-53ae-4a5b-99eb-9d07229626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = m.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011c9351-4758-4f80-b121-b960d7db29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnn_weight = weight.permute(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f5dd3ce-8a22-4ffd-976e-83edc9d5746a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 12]' is invalid for input of size 288",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mn_weight \u001b[38;5;241m=\u001b[39m \u001b[43mpnn_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[5, 12]' is invalid for input of size 288"
     ]
    }
   ],
   "source": [
    "mn_weight = pnn_weight.reshape([5,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67336141-9366-463c-a117-0cc09179fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape 到m * n 并transpose\n",
    "weight = torch.transpose(weight.reshape([5,12]),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9632674e-e85f-4d20-8efa-760b32440882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1520,  0.2060,  0.2284, -0.1810,  0.2607],\n",
       "        [ 0.0295, -0.0378,  0.0138,  0.2794,  0.1455],\n",
       "        [ 0.0082, -0.2350,  0.1256,  0.0217,  0.1567],\n",
       "        [ 0.0489, -0.0637,  0.1744,  0.1335,  0.0948],\n",
       "        [-0.0311,  0.2040, -0.1192, -0.1462, -0.0013],\n",
       "        [-0.0947,  0.1130, -0.1895, -0.0385,  0.0461],\n",
       "        [ 0.1862, -0.0235, -0.2529,  0.2474, -0.1296],\n",
       "        [-0.0883,  0.1721,  0.1799,  0.1601, -0.1431],\n",
       "        [ 0.0591, -0.1370,  0.2683, -0.0775,  0.1635],\n",
       "        [ 0.1586, -0.1273, -0.0922, -0.1727,  0.0998],\n",
       "        [ 0.1335,  0.0992,  0.0145, -0.1227,  0.2118],\n",
       "        [-0.1693,  0.1448,  0.0321,  0.2753, -0.0215]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e249885-9d89-4bbc-8674-e58eb25ce480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1520, -0.0311,  0.0591,  0.0295, -0.0947,  0.1586,  0.0082,  0.1862,\n",
       "          0.1335,  0.0489, -0.0883, -0.1693],\n",
       "        [ 0.2060,  0.2040, -0.1370, -0.0378,  0.1130, -0.1273, -0.2350, -0.0235,\n",
       "          0.0992, -0.0637,  0.1721,  0.1448],\n",
       "        [ 0.2284, -0.1192,  0.2683,  0.0138, -0.1895, -0.0922,  0.1256, -0.2529,\n",
       "          0.0145,  0.1744,  0.1799,  0.0321],\n",
       "        [-0.1810, -0.1462, -0.0775,  0.2794, -0.0385, -0.1727,  0.0217,  0.2474,\n",
       "         -0.1227,  0.1335,  0.1601,  0.2753],\n",
       "        [ 0.2607, -0.0013,  0.1635,  0.1455,  0.0461,  0.0998,  0.1567, -0.1296,\n",
       "          0.2118,  0.0948, -0.1431, -0.0215]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd56d7-7463-4aa1-b819-af5e3a71d3f1",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa504312-9860-43f4-9ad1-ba8c745221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper as oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cc8b0-4503-4d87-b1cf-73413363fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = oh.make_node('ConvTranspose',inputs='data',outputs=['output'],name='ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accd2ac9-4d94-4af5-8eb7-9b62577f758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"/Users/bob/code/CodeReading/ppl.nn/cmake-build-debug/samples/cpp/run_model/conv_transpose.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a92655a5-4c28-4828-a04c-dc53591a98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e18d5ac9-1d4d-4a5c-b7ba-37091479a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del graph.initializer[0].dims[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e906aca3-a1fe-4745-bb09-9dc8b9a3ea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 16, 3, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[33,16,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8add7045-cbf8-453c-a60d-3676a31f0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.initializer[0].dims.extend([33,16,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d95ff018-0cd8-49bd-8942-b98b725c12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save_model(model,\"/Users/bob/code/CodeReading/ppl.nn/cmake-build-debug/samples/cpp/run_model/modified.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec672f2-c4b1-4db5-896b-0fb99c2ba471",
   "metadata": {},
   "source": [
    "### Conv2DShapeinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda1ae3f-4f29-43c4-af46-0223cd57385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509334b5-ab6f-4cd2-933d-46c308632241",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.conv2d(\n",
    "    tf.random.uniform([1,9,9,3]), # input\n",
    "    tf.random.uniform([1,1,3,2]), #  filter, hwio ,tf\n",
    "    strides=[3,3], # strides\n",
    "    padding = \"SAME\", # padding\n",
    "    dilations = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73531e48-16d7-4878-91fb-84fee0752a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4014d9f6-b322-4b50-ba97-85bad5c9eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = tf.nn.conv_transpose(\n",
    "    tf.random.uniform([1,3,3,2]),\n",
    "    tf.random.uniform([2,2,3,2]),\n",
    "    [1,7,7,3],\n",
    "    strides=[3,3],\n",
    "    padding = \"SAME\", # padding\n",
    "    dilations = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efdc5fb3-eb97-4d99-90da-0f6e4e3152e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7, 7, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0a150-735b-448c-83cf-528882d49929",
   "metadata": {},
   "source": [
    "### InputGradTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "902a5d9a-2303-46dd-9772-b45f3a0b288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(shape, begin = 0, truncate = False, truncatBase =13):\n",
    "    # np.array().astype(np.float32).reshape(data_shape)\n",
    "    size = reduce(operator.mul, shape, 1)\n",
    "    data = []\n",
    "    for i in range(size):\n",
    "        if not truncate:\n",
    "            data.append(i + begin);\n",
    "        else:\n",
    "            tmp =  (i + begin) if (i + begin) < 0 else (i + begin) % truncateBase;\n",
    "            data.append(tmp)\n",
    "    return np.array(data).astype(np.float32).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d98a209-c8c2-4e29-892e-388de3c93078",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_transpose = np.array([0,1,2,3,0,1],dtype=np.float32).reshape([2,1,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1549cf3-f224-4a37-aaec-6d87b4ac5ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 1., 2.]]],\n",
       "\n",
       "\n",
       "       [[[3., 0., 1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62000ea-d967-4efb-8424-584c9a558b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_transpose = np.transpose(kernel_transpose,[1,2,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52cd5733-f831-4365-89de-5639f72a504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 3.],\n",
       "         [1., 0.],\n",
       "         [2., 1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40458a3f-af60-44f0-ba42-259782c86377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_transpose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e66087c-2d55-4610-8098-0b6e28d04e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_grad = np.array([   0,\n",
    "   1,\n",
    "   2,\n",
    "   3,\n",
    "   4,\n",
    "   0,\n",
    "   1,\n",
    "   2,\n",
    "   3,\n",
    "   4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4],dtype=np.float32).reshape([1,9,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a72844-037b-4a17-a798-6bfc240b724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 5, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91c571cb-cda2-4678-8a3d-30ab7ee0482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 18:46:04.637762: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-16 18:46:04.756624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7feb86ba65a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-09-16 18:46:04.756640: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "test_output = tf.nn.conv_transpose(\n",
    "    output_grad,\n",
    "    kernel_transpose,\n",
    "    [1,9,9,3],\n",
    "    strides=[1,2],\n",
    "    padding = \"SAME\", # padding\n",
    "    dilations = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dad44c-e830-424b-a135-70cf2131cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955da626-530b-44b3-84a5-0d1c161f1d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_transpose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7944f7b3-dd0e-4e37-850c-2d457c6ca24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 3.],\n",
       "         [1., 0.],\n",
       "         [2., 1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "55f5c0c2-ea31-439e-a978-4e6df31e7d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 9, 3)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93bdafd9-6ce8-4f50-ab24-0d230f6b5238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.numpy().flatten()[78]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5bd75a-afe5-479d-b265-2289e37f15df",
   "metadata": {},
   "source": [
    "### TorchWithBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da48aeca-249d-4564-a5b8-28151282d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5cb490-69ad-40b1-8ddc-567bd2a06d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699de6e8-2208-47f4-96e1-f47d426fbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_bias = nn.ConvTranspose2d(2, 3, 2, stride=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04452a5e-d930-4c6b-958e-726ddc7f5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_inp1 = torch.rand(1,2,7,7)\n",
    "conv_inp2 = torch.rand(1,2,8,8)\n",
    "\n",
    "conv1 = torch.nn.Conv2d(2, 4, kernel_size = 3, stride = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9227ee11-bf58-4930-a46a-e73f9f058938",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = conv1(conv_inp1)     \n",
    "out2 = conv1(conv_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "19d873ed-8520-4988-8781-962bcae76e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f505e636-fd93-42ba-8ee0-302229744179",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_t1 = torch.nn.ConvTranspose2d(4, 2, kernel_size=3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7ba24b13-3dc9-430d-aad7-58de5761aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_t2 = torch.nn.ConvTranspose2d(4, 2, kernel_size=3, stride=2, output_padding=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a985300c-d378-4577-bad4-bb4c4d84655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed2 = conv_t2(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0926bf90-bb34-4aec-9b2b-ff4cc0de6e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0806, -0.0169], requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_t2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a79e70c6-6d85-47df-8608-4c7e603fad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_t2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6a17529a-060f-4a4a-a6d2-6ba738d4c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_weight_to_pnn(weight):\n",
    "    res = weight.detach()\n",
    "    return res.permute([0,2,3,1]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ac0f8dba-6551-45a2-99b1-fb839ffa2917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08598892,  0.22859292,  0.0242203 , -0.22505076, -0.19747889,\n",
       "       -0.16396557, -0.13732672, -0.16988268, -0.19650199, -0.11784714,\n",
       "       -0.09573036, -0.09520026,  0.20134456, -0.0902171 , -0.07027142,\n",
       "        0.12464471,  0.02211244, -0.14829227, -0.13545354, -0.10666129,\n",
       "        0.16517378,  0.16190787, -0.22098807,  0.04119961,  0.1871757 ,\n",
       "       -0.18650287,  0.10129328,  0.01875649,  0.18923567,  0.00457549,\n",
       "        0.07432802,  0.08531667, -0.08495605,  0.04637216,  0.18991925,\n",
       "       -0.06346714, -0.18734127, -0.01229569, -0.05700004,  0.01039293,\n",
       "       -0.03600515,  0.2074878 , -0.23093118, -0.02857821, -0.2168017 ,\n",
       "        0.1478384 , -0.05225842,  0.0383354 ,  0.10759346,  0.06498723,\n",
       "       -0.20631063, -0.02114594,  0.17599969, -0.10371006,  0.06678195,\n",
       "        0.22403146, -0.09102502, -0.15165281, -0.17513567, -0.16076994,\n",
       "        0.20458682, -0.17611957, -0.11416935,  0.12211417,  0.18790372,\n",
       "        0.0197589 , -0.16182375,  0.17027919, -0.22771728, -0.23003216,\n",
       "       -0.087295  , -0.03678715], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_weight_to_pnn(conv_t2.weight).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "38be8730-d91d-4dd6-af78-be3d7a025b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_input = torch.from_numpy(np.array(get_data(1*4*3*3),dtype=np.float32).reshape(1,4,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e000a509-e536-4033-a30f-8212e57a6bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e22bb902-4e91-4d30-9060-61a56aeadc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = conv_t2(bias_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b0126b49-8fc7-4fdc-91ba-252ada1791fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8, 8])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "afb00735-55cf-487a-b9c0-9bcb1d2f193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07447739, -0.13594478, -0.16773763,  0.23172377,  0.15555386,\n",
       "        0.01324239, -0.21690345, -0.19830418,  0.21050198,  0.15979357,\n",
       "        0.20807673, -0.03992857,  0.10459785, -0.1875741 , -0.16358006,\n",
       "       -0.22946231,  0.0553066 ,  0.04950316,  0.11889558,  0.21686982,\n",
       "        0.13792156,  0.00949842,  0.16429333, -0.081011  , -0.10801502,\n",
       "        0.09868048, -0.0355915 ,  0.21629916,  0.04159497, -0.06480502,\n",
       "        0.23078342,  0.14525475,  0.03220336, -0.00782782,  0.11145274,\n",
       "        0.21582536,  0.01092982, -0.02478327,  0.05318256, -0.1060877 ,\n",
       "       -0.1375268 ,  0.22525497, -0.06433952,  0.07188036,  0.17733468,\n",
       "        0.03209467,  0.14347185,  0.19502284, -0.22364798,  0.1730703 ,\n",
       "       -0.11489874, -0.14852381, -0.00231485, -0.13392021,  0.1441349 ,\n",
       "        0.06693746,  0.19474347,  0.15590934, -0.02650508, -0.2162298 ,\n",
       "        0.08019136,  0.15587641, -0.09580086,  0.142491  , -0.05759637,\n",
       "        0.18616466, -0.06096216, -0.1799687 , -0.0028241 , -0.16578202,\n",
       "        0.07479195, -0.05810347], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_weight_to_pnn(conv_t1.weight).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "720007ab-6605-4d1f-8d53-2a4953d2208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2 = conv_t1(bias_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0c92e918-da62-4ff3-b024-5baeeba4d10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 7, 7])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8a10f081-a4f7-42b6-a5eb-dc78c1174bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1756, -0.1742], requires_grad=True)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_t1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2c206-ad76-42f1-96aa-7094cdb3cf09",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71a4b3-aa77-4c6b-abb0-99fee8197da0",
   "metadata": {},
   "source": [
    "#### Case1_Pad_SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6cbfccf4-a9e3-46b3-972e-d6df7662ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = [1,9,9,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b857a5c5-50f0-4df4-8f9f-932404ed062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(get_data(reduce(operator.mul, data_shape, 1))).astype(np.float32).reshape(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4edf9757-6d93-4dd8-b36d-2ab5ad9952f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_shape = [2,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "86b32529-54f6-4d49-9cbc-a5dc55ae18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array(get_data(reduce(operator.mul, filter_shape, 1))).astype(np.float32).reshape(filter_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "69b8f9e7-f807-4c21-983e-3830aa7cdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = tf.nn.conv2d(tf.convert_to_tensor(data), tf.convert_to_tensor(weight), strides=1,dilations=2,padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8104b1ae-b9c3-4978-9465-7ae5ba90ebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 9, 9, 5])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e85738e8-f80d-4926-97b2-f82e70b350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "t_data = np.array(get_data(reduce(operator.mul, conv_output.shape, 1))).astype(np.float32).reshape(conv_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "735c1a1d-032a-41a6-923e-892abeedcb32",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1. [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [272]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:2560\u001b[0m, in \u001b[0;36mconv2d_transpose_v2\u001b[0;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2557\u001b[0m strides \u001b[38;5;241m=\u001b[39m _get_sequence(strides, \u001b[38;5;241m2\u001b[39m, channel_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2558\u001b[0m dilations \u001b[38;5;241m=\u001b[39m _get_sequence(dilations, \u001b[38;5;241m2\u001b[39m, channel_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdilations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_backprop_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_backprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py:1259\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1257\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1259\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv2d_backprop_input_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_backprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1265\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py:1353\u001b[0m, in \u001b[0;36mconv2d_backprop_input_eager_fallback\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1349\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [input_sizes, \u001b[38;5;28mfilter\u001b[39m, out_backprop]\n\u001b[1;32m   1350\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m\"\u001b[39m, strides, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cudnn_on_gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1351\u001b[0m use_cudnn_on_gpu, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicit_paddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1352\u001b[0m explicit_paddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_format, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdilations\u001b[39m\u001b[38;5;124m\"\u001b[39m, dilations)\n\u001b[0;32m-> 1353\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConv2DBackpropInput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   1356\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   1357\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2DBackpropInput\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1. [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "output = tf.nn.conv2d_transpose(t_data,weight,[1,9,9,3],strides=1,dilations=2,padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f693e28d-aa56-4127-b5e8-0e88c0eddaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 9, 9, 3])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1549867f-dc00-4562-95aa-5be79dff1e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   30.,    80.,   130.,   180.,   230.,   280.,    80.,   255.,\n",
       "         430.,   605.,   780.,   955.,   130.,   430.,   730.,  1030.,\n",
       "        1330.,  1630.,   180.,   605.,  1030.,  1455.,  1880.,  2305.,\n",
       "         230.,   780.,  1330.,   330.,   380.,   430.,   480.,   530.,\n",
       "         580.,  1130.,  1305.,  1480.,  1655.,  1830.,  2005.,  1930.,\n",
       "        2230.,  2530.,  2830.,  3130.,  3430.,  2730.,  3155.,  3580.,\n",
       "        4005.,  4430.,  4855.,  3530.,  4080.,  4630.,   280.,   955.,\n",
       "        1630.,  2305.,  2980.,  3655.,   330.,  1130.,  1930.,  2730.,\n",
       "        3530.,  4330.,   380.,  1305.,  2230.,  3155.,  4080.,  5005.,\n",
       "         430.,  1480.,  2530.,  3580.,  4630.,  5680.,   480.,  1655.,\n",
       "        2830.,  4330.,  5005.,  5680.,  6355.,  7030.,  7705.,  5130.,\n",
       "        5930.,  6730.,  7530.,  8330.,  9130.,  5930.,  6855.,  7780.,\n",
       "        8705.,  9630., 10555.,  6730.,  7780.,  8830.,  9880., 10930.,\n",
       "       11980.,  7530.,  8705.,  9880.,   530.,  1830.,  3130.,  4430.,\n",
       "        5730.,  7030.,   580.,  2005.,  3430.,  4855.,  6280.,  7705.,\n",
       "         630.,  2180.,  3730.,  5280.,  6830.,  8380.,   680.,  2355.,\n",
       "        4030.,  5705.,  7380.,  9055.,   730.,  2530.,  4330.,  8330.,\n",
       "        9630., 10930., 12230., 13530., 14830.,  9130., 10555., 11980.,\n",
       "       13405., 14830., 16255.,  9930., 11480., 13030., 14580., 16130.,\n",
       "       17680., 10730., 12405., 14080., 15755., 17430., 19105., 11530.,\n",
       "       13330., 15130.,   780.,  2705.,  4630.,  6555.,  8480., 10405.,\n",
       "         830.,  2880.,  4930.,  6980.,  9030., 11080.,   880.,  3055.,\n",
       "        5230.,  7405.,  9580., 11755.,   930.,  3230.,  5530.,  7830.,\n",
       "       10130., 12430.,   980.,  3405.,  5830., 12330., 14255., 16180.,\n",
       "       18105., 20030., 21955., 13130., 15180., 17230., 19280., 21330.,\n",
       "       23380., 13930., 16105., 18280., 20455., 22630., 24805., 14730.,\n",
       "       17030., 19330., 21630., 23930., 26230., 15530., 17955., 20380.,\n",
       "        1030.,  3580.,  6130.,  8680., 11230., 13780.,  1080.,  3755.,\n",
       "        6430.,  9105., 11780., 14455.,  1130.,  3930.,  6730.,  9530.,\n",
       "       12330., 15130.,  1180.,  4105.,  7030.,  9955., 12880., 15805.,\n",
       "        1230.,  4280.,  7330.], dtype=float32)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e9472b4d-ec9f-40da-9f3c-d0d2216d7bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55.,  1.,\n",
       "        6., 11., 16., 21., 26., 31., 36., 41., 46., 51., 56.,  2.,  7.,\n",
       "       12., 17., 22., 27., 32., 37., 42., 47., 52., 57.,  3.,  8., 13.,\n",
       "       18., 23., 28., 33., 38., 43., 48., 53., 58.,  4.,  9., 14., 19.,\n",
       "       24., 29., 34., 39., 44., 49., 54., 59.], dtype=float32)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.transpose([3,0,1,2]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d4d078b6-919d-4481-8260-c18b8029c24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3, 5)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a03b3-430f-454a-9488-b3ff5873c806",
   "metadata": {},
   "source": [
    "### Torch dilation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "9fe325d9-259c-48c9-a7ee-611f425f6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchw_input = get_data([1,3,8,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c0033-0d76-4906-9410-c06886c4ecf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "3b4302fd-caec-466d-9c7d-2cf82cdfafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(3, 4, kernel_size = [2,1], stride = [1,1],dilation=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "3e0ad0cf-0dec-41ef-aed0-0553d7542ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv(torch.from_numpy(nchw_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c0c30b12-4f7a-4818-bc7e-0b7604078dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 7, 8])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6604e4c4-0830-4d6e-bd25-bb6355349d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_input = get_data(list(conv_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "eed52854-669d-4dfd-bda7-948d56d8a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_t = torch.nn.ConvTranspose2d(4,3,kernel_size=[2,1],stride=[1,1],dilation=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "7983fa0a-1c14-4efe-9db5-dffb712a9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv_t(torch.from_numpy(t_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3ef16079-6c96-46f4-bc51-70afac48ff3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 8, 8)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "dc09eb21-bee3-4542-a8d9-43f40989f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28062713,  0.30849963,  0.39423364,  0.14346242, -0.35381427,\n",
       "       -0.06700224,  0.11777908, -0.00255701,  0.29896873,  0.12235141,\n",
       "        0.32177818,  0.15091985,  0.00405869,  0.3991269 ,  0.34511155,\n",
       "        0.2601741 , -0.03359014,  0.0280278 , -0.07241759,  0.12268496,\n",
       "        0.13441616, -0.34851736, -0.28691405, -0.19857115], dtype=float32)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_weight_to_pnn(conv_t.weight).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6ccd4de3-c4b5-4e9e-8e97-72ad04dfabf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 7, 8)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b0ac691b-7db5-4b65-a4ed-3cb56001eaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.1159534,  -4.785906 ,  -4.4558587,  -4.1258116,  -3.795764 ,\n",
       "        -3.4657168,  -3.1356685,  -2.8056223, -25.035315 , -24.527794 ,\n",
       "       -24.02028  , -23.51276  , -23.00524  , -22.497725 , -21.990206 ,\n",
       "       -21.482689 , -20.97517  , -20.467653 , -19.960133 , -19.452616 ,\n",
       "       -18.945099 , -18.43758  , -17.930065 , -17.422544 , -16.91503  ,\n",
       "       -16.40751  , -15.899991 , -15.392475 , -14.884956 , -14.3774395,\n",
       "       -13.869921 , -13.362402 , -12.854884 , -12.347364 , -11.839849 ,\n",
       "       -11.332328 , -10.824816 , -10.317295 ,  -9.809777 ,  -9.302259 ,\n",
       "        -8.794743 ,  -8.287225 ,  -7.779705 ,  -7.27219  ,  -6.7646704,\n",
       "        -6.2571507,  -5.7496347,  -5.242114 ,  -4.7346   ,  -4.2270794,\n",
       "        -3.7195606,  -3.2120447,  -2.704526 ,  -2.197011 ,  -1.6894913,\n",
       "        -1.1819754, -14.041149 , -13.863682 , -13.686211 , -13.508739 ,\n",
       "       -13.331268 , -13.153797 , -12.97633  , -12.798859 ,  65.17009  ,\n",
       "        65.99785  ,  66.8256   ,  67.65336  ,  68.48111  ,  69.30886  ,\n",
       "        70.13662  ,  70.96437  ,  37.84805  ,  38.32326  ,  38.79848  ,\n",
       "        39.273693 ,  39.7489   ,  40.22412  ,  40.699333 ,  41.174553 ,\n",
       "        41.64976  ,  42.12498  ,  42.60019  ,  43.07541  ,  43.55062  ,\n",
       "        44.025833 ,  44.50105  ,  44.97626  ,  45.45147  ,  45.92669  ,\n",
       "        46.40191  ,  46.87712  ,  47.35233  ,  47.82755  ,  48.302757 ,\n",
       "        48.777977 ,  49.25319  ,  49.7284   ,  50.203617 ,  50.67883  ,\n",
       "        51.15405  ,  51.629257 ,  52.104477 ,  52.57969  ,  53.054897 ,\n",
       "        53.530117 ,  54.00533  ,  54.48055  ,  54.955757 ,  55.430977 ,\n",
       "        55.906185 ,  56.381405 ,  56.856617 ,  57.33183  ,  57.807045 ,\n",
       "        58.282257 ,  58.757465 ,  59.232693 ,  59.707897 ,  60.183125 ,\n",
       "       -50.866013 , -51.21855  , -51.57109  , -51.923634 , -52.276173 ,\n",
       "       -52.628716 , -52.981255 , -53.333794 ,  77.976654 ,  79.14939  ,\n",
       "        80.32212  ,  81.49485  ,  82.66758  ,  83.84031  ,  85.01304  ,\n",
       "        86.18577  ,  65.58917  ,  66.67528  ,  67.761375 ,  68.84748  ,\n",
       "        69.933586 ,  71.01969  ,  72.105804 ,  73.1919   ,  74.27801  ,\n",
       "        75.36411  ,  76.45022  ,  77.536316 ,  78.62242  ,  79.70853  ,\n",
       "        80.79463  ,  81.88074  ,  82.96684  ,  84.05295  ,  85.13905  ,\n",
       "        86.22516  ,  87.311264 ,  88.397354 ,  89.48347  ,  90.56957  ,\n",
       "        91.65568  ,  92.74178  ,  93.82788  ,  94.913994 ,  96.0001   ,\n",
       "        97.0862   ,  98.1723   ,  99.25841  , 100.34451  , 101.43062  ,\n",
       "       102.51672  , 103.60283  , 104.68893  , 105.77503  , 106.86114  ,\n",
       "       107.94724  , 109.03334  , 110.119446 , 111.20555  , 112.29166  ,\n",
       "       113.37776  , 114.46387  , 115.54997  , 116.63607  , -25.92736  ,\n",
       "       -26.013988 , -26.100615 , -26.187239 , -26.273863 , -26.36049  ,\n",
       "       -26.447115 , -26.533743 ], dtype=float32)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1d0f09fb-fc69-43db-8be0-f57a1ba3bf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2103, -0.0517, -0.2749], requires_grad=True)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_t.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7e87c-bbaf-477f-af83-09147cde61cb",
   "metadata": {},
   "source": [
    "### Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38c89b5-51c7-4b8c-a1b6-1ae4e46a2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_1 = torch.rand([1,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6874ff-8bd6-4d00-b4d1-65479c302287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4863451 , 0.5473978 , 0.5295979 , 0.99181163, 0.83747786],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_1.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8629440-7d5b-4ebb-93b7-9d6e54a44cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_2 = torch.rand([1,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d96c63-8ecc-44a2-8a45-9fed9ff61a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44318914, 0.26870042, 0.37088585, 0.4972793 , 0.82523   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_2.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e13a5f-6c64-45ad-8762-cd56cac1fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92953426, 0.8160982 , 0.9004837 , 1.4890909 , 1.6627078 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(add_1 + add_2).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0768e394-af51-4d5c-a9a6-43b4141cf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552d676d-276e-498e-9951-be4e372c8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.ascontiguousarray(foo.transpose([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98371911-a5a3-465a-9589-1c371f7cf87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.flags['C_CONTIGUOUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71a09a39-cb8b-45b6-9824-49ecf3adb331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.flags['C_CONTIGUOUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af12abd3-138d-4854-8660-1c11dffb06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([i for i in range(1,2*3*3*4 + 1)]).reshape((2,3,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdbf0673-96bb-4261-b49b-1b05a8549401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "366d0659-3ae8-4c7b-b2d0-31c6cbd279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(a, (3,1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6217baea-10e3-4072-871a-1d5340aae23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1, 37],\n",
       "         [ 5, 41],\n",
       "         [ 9, 45]],\n",
       "\n",
       "        [[13, 49],\n",
       "         [17, 53],\n",
       "         [21, 57]],\n",
       "\n",
       "        [[25, 61],\n",
       "         [29, 65],\n",
       "         [33, 69]]],\n",
       "\n",
       "\n",
       "       [[[ 2, 38],\n",
       "         [ 6, 42],\n",
       "         [10, 46]],\n",
       "\n",
       "        [[14, 50],\n",
       "         [18, 54],\n",
       "         [22, 58]],\n",
       "\n",
       "        [[26, 62],\n",
       "         [30, 66],\n",
       "         [34, 70]]],\n",
       "\n",
       "\n",
       "       [[[ 3, 39],\n",
       "         [ 7, 43],\n",
       "         [11, 47]],\n",
       "\n",
       "        [[15, 51],\n",
       "         [19, 55],\n",
       "         [23, 59]],\n",
       "\n",
       "        [[27, 63],\n",
       "         [31, 67],\n",
       "         [35, 71]]],\n",
       "\n",
       "\n",
       "       [[[ 4, 40],\n",
       "         [ 8, 44],\n",
       "         [12, 48]],\n",
       "\n",
       "        [[16, 52],\n",
       "         [20, 56],\n",
       "         [24, 60]],\n",
       "\n",
       "        [[28, 64],\n",
       "         [32, 68],\n",
       "         [36, 72]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f509c3b-efd8-4fa3-a7fb-4688259735a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e97d82-90f9-421c-bebd-4d2ce8568ddc",
   "metadata": {},
   "source": [
    "### 5x5s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e858b69-f2af-4f23-b65b-cde54be00c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.nn.Conv2d(32,4,(5,5),stride=1,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2bf0c66-f600-4c46-9940-4010c408c9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(32, 4, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45d00b54-50ce-4614-98ae-c894eaa1d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 32, 416, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eadda9e9-80d2-4a70-8541-8ec922e3fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f6291d4-d7af-4929-8975-beff33db3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  data,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/Users/bob/docs/ByteDance/PNN/converter/dwm/dwm_5x5s1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['data'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c245975-1ef1-4a08-8dd0-a6beca1be290",
   "metadata": {},
   "source": [
    "### 3x3s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b54a668-1862-4af9-b0f5-b7351e1aaffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Conv2d(32,4,(3,3),stride=1,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06215c8e-d2df-44e6-b033-962e0f6bd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 32, 416, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0da80e4e-e4b1-44be-a1ad-830f493ae020",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3098d878-64df-4c70-b719-48f955df940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  data,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/Users/bob/docs/ByteDance/PNN/converter/dwm/dwm_3x3s1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['data'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592a44-fe65-461f-b9b5-c290110c4799",
   "metadata": {},
   "source": [
    "### 5x5s1s1-14x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b125a217-7950-4856-9255-b0e19d6031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = torch.nn.Conv2d(256,256,(5,5),stride=1,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4609b8-7514-483f-984f-8ad780a58ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 256, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c4f231-a029-4e52-adad-06fedbba66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = small_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d857775-9a73-49b3-b14c-34c64b3d1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(small_model,               # model being run\n",
    "                  data,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/Users/bob/docs/ByteDance/PNN/converter/dwm/dwm_5x5s1_small.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['data'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796e3f3-462a-47fd-8914-74a309e0b067",
   "metadata": {},
   "source": [
    "### 7x7s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1c971a-1739-4ac4-90a5-0edbbd46bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 32, 416, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8306ff-8bd2-4247-a393-d3b4152b11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.nn.Conv2d(32,4,(7,7),stride=1,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100b96fc-b4b4-42ad-8c17-d6ec40079b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2e9b20-fd09-4a22-89bd-a091dfab6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  data,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"/Users/bob/docs/ByteDance/PNN/converter/dwm/dwm_7x7s1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['data'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083b578-0e87-4799-9950-2a9975a40675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
