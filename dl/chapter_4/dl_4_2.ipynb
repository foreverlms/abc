{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "    \n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "    \n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "        \n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "                                       \n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "        \n",
    "        \n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "        \n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "    \n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "        \n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "        \n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(4,3),nn.ReLU(),nn.Linear(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.rand(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=net(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1928, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyModule,self).__init__(**kwargs);\n",
    "        self.weight = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight1=torch.rand(20,20)\n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[0.1239, 0.3848, 0.8373, 0.6588, 0.7113, 0.8705, 0.0796, 0.3825, 0.1711,\n",
      "         0.9441, 0.7956, 0.1103, 0.0068, 0.9385, 0.6255, 0.7774, 0.9924, 0.3567,\n",
      "         0.7544, 0.0140],\n",
      "        [0.1768, 0.0403, 0.8012, 0.2789, 0.8994, 0.3911, 0.9659, 0.9159, 0.7282,\n",
      "         0.1771, 0.9174, 0.5610, 0.5464, 0.0122, 0.8337, 0.0148, 0.7537, 0.5281,\n",
      "         0.5309, 0.8504],\n",
      "        [0.8882, 0.0131, 0.0799, 0.5777, 0.8364, 0.1469, 0.4844, 0.9128, 0.9627,\n",
      "         0.0274, 0.5378, 0.4283, 0.8872, 0.1107, 0.6105, 0.2327, 0.7091, 0.1408,\n",
      "         0.8121, 0.7699],\n",
      "        [0.4932, 0.4027, 0.0991, 0.3281, 0.5955, 0.3658, 0.5784, 0.3158, 0.6725,\n",
      "         0.9345, 0.1733, 0.8438, 0.6291, 0.9810, 0.7884, 0.5437, 0.2490, 0.4427,\n",
      "         0.8027, 0.4939],\n",
      "        [0.8799, 0.8934, 0.4125, 0.9576, 0.7691, 0.9591, 0.4713, 0.1658, 0.3762,\n",
      "         0.1995, 0.5198, 0.8160, 0.2834, 0.1425, 0.1825, 0.2042, 0.9459, 0.1795,\n",
      "         0.9083, 0.3752],\n",
      "        [0.7194, 0.3767, 0.2725, 0.3684, 0.2505, 0.6033, 0.9893, 0.9804, 0.3875,\n",
      "         0.4112, 0.0849, 0.1828, 0.9588, 0.9192, 0.6351, 0.2989, 0.4788, 0.4304,\n",
      "         0.4712, 0.1338],\n",
      "        [0.0948, 0.7545, 0.1581, 0.2918, 0.1092, 0.0597, 0.1789, 0.1946, 0.6058,\n",
      "         0.4225, 0.8814, 0.2725, 0.4626, 0.5925, 0.9646, 0.2210, 0.0811, 0.0440,\n",
      "         0.8129, 0.4286],\n",
      "        [0.3820, 0.0952, 0.1503, 0.5325, 0.5871, 0.3963, 0.6027, 0.5141, 0.0630,\n",
      "         0.8563, 0.1193, 0.1933, 0.6534, 0.5709, 0.8286, 0.6856, 0.3710, 0.1094,\n",
      "         0.1641, 0.7449],\n",
      "        [0.6163, 0.3818, 0.5385, 0.4499, 0.7600, 0.9741, 0.4953, 0.0148, 0.7902,\n",
      "         0.6224, 0.6494, 0.6694, 0.0507, 0.6940, 0.4059, 0.4504, 0.8766, 0.0022,\n",
      "         0.1794, 0.8802],\n",
      "        [0.4690, 0.3994, 0.0709, 0.3221, 0.1340, 0.3154, 0.2993, 0.7078, 0.8299,\n",
      "         0.7457, 0.3032, 0.1634, 0.0991, 0.0363, 0.0623, 0.7016, 0.1239, 0.2927,\n",
      "         0.1977, 0.0144],\n",
      "        [0.6780, 0.3547, 0.6573, 0.4996, 0.5666, 0.6403, 0.2928, 0.6296, 0.3671,\n",
      "         0.4965, 0.7362, 0.4780, 0.3971, 0.0683, 0.1300, 0.2610, 0.0488, 0.2885,\n",
      "         0.9355, 0.5173],\n",
      "        [0.7884, 0.9653, 0.0960, 0.9522, 0.5108, 0.5419, 0.2954, 0.5906, 0.3710,\n",
      "         0.9521, 0.7798, 0.1474, 0.8444, 0.6398, 0.8143, 0.7523, 0.0870, 0.8265,\n",
      "         0.6860, 0.0410],\n",
      "        [0.9251, 0.4112, 0.2199, 0.9283, 0.7809, 0.0835, 0.7300, 0.8914, 0.1851,\n",
      "         0.8345, 0.5505, 0.3233, 0.9119, 0.7630, 0.8206, 0.8438, 0.7190, 0.2949,\n",
      "         0.7560, 0.3473],\n",
      "        [0.5913, 0.7598, 0.2994, 0.2158, 0.6283, 0.0371, 0.9997, 0.7818, 0.3205,\n",
      "         0.6794, 0.7725, 0.7595, 0.0479, 0.9525, 0.5013, 0.2185, 0.4160, 0.0241,\n",
      "         0.9672, 0.3889],\n",
      "        [0.5029, 0.2288, 0.5905, 0.9608, 0.2908, 0.2192, 0.0186, 0.4908, 0.4184,\n",
      "         0.9852, 0.2980, 0.6226, 0.2684, 0.2055, 0.4333, 0.6361, 0.0682, 0.3574,\n",
      "         0.2353, 0.6304],\n",
      "        [0.6685, 0.7386, 0.7330, 0.8215, 0.6928, 0.3384, 0.6179, 0.7314, 0.3022,\n",
      "         0.8457, 0.7433, 0.4549, 0.9872, 0.8202, 0.0324, 0.6226, 0.8995, 0.4809,\n",
      "         0.4228, 0.9633],\n",
      "        [0.8556, 0.7311, 0.0866, 0.1704, 0.6921, 0.7665, 0.8190, 0.4132, 0.8256,\n",
      "         0.7575, 0.5575, 0.0473, 0.0046, 0.6949, 0.9500, 0.7780, 0.6044, 0.0927,\n",
      "         0.0215, 0.6320],\n",
      "        [0.1754, 0.7747, 0.6624, 0.2887, 0.8240, 0.7323, 0.4112, 0.8300, 0.8618,\n",
      "         0.8460, 0.9755, 0.9506, 0.4482, 0.5324, 0.2325, 0.0623, 0.7628, 0.1243,\n",
      "         0.9116, 0.5965],\n",
      "        [0.7561, 0.2843, 0.5893, 0.1404, 0.7741, 0.4974, 0.3590, 0.9511, 0.0347,\n",
      "         0.6517, 0.4873, 0.7366, 0.9828, 0.4418, 0.8987, 0.7694, 0.8356, 0.9968,\n",
      "         0.6721, 0.1301],\n",
      "        [0.0934, 0.6793, 0.7818, 0.9537, 0.5926, 0.8143, 0.7490, 0.8140, 0.5076,\n",
      "         0.0255, 0.5410, 0.1346, 0.5674, 0.0402, 0.1674, 0.7115, 0.8915, 0.7029,\n",
      "         0.8408, 0.0264]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "n=MyModule()\n",
    "for name in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1,1,bias=False)\n",
    "net = nn.Sequential(linear,linear)\n",
    "print(net\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面可以显示在这个Sequential中，两个线性层共享了一个权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.ones(1,1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=net(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.ones(2,2,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tmp+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.nn.Linear(1,1)\n",
    "init.constant_(l.weight,val=3)\n",
    "init.constant_(l.bias,val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.ones(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 \n",
    "4.2.4关于参数共享，两个线性层共用参数，对于权重的梯度应该是错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = l(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = l(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出第一次经过传播后，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
