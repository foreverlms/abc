{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from d2l import d2l_en as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X,valid_len):\n",
    "    if valid_len is None:\n",
    "        return nn.functional.softmax(X,dim=1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_len.dim() == 1:\n",
    "            valid_len = torch.repeat_interleave(valid_len,repeats=shape[1],dim=0)\n",
    "        else:\n",
    "            valid_len = valid_len.reshape(-1)\n",
    "        X = d2l.sequence_mask(X.reshape(-1,shape[-1]),valid_len,value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3649, 0.6351, 0.0000, 0.0000],\n",
       "         [0.5457, 0.4543, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3728, 0.4230, 0.2042, 0.0000],\n",
       "         [0.3328, 0.2840, 0.3833, 0.0000]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2,2,4),torch.tensor([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3.]],\n",
       "\n",
       "        [[3., 3.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(torch.ones(2,1,3),torch.ones(2,3,2)) # X(b,n,m) * Y(b,m,k) -> Result(b,n,k) 批量点乘 n个m维向量，每个都与k个m维向量做点积，这正合q*k，一次性把每个Q做完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self,dropout,**kwargs):\n",
    "        super(DotProductAttention,self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    #q -> batch_size,query_size,query_d/d/embedding_size\n",
    "    #k -> \n",
    "    def forward(self,query,key,value,valid_len=None):\n",
    "        d = query.shape[-1]#d就是指embeddings的维度吧？\n",
    "        \n",
    "        #scaled dot-production attention\n",
    "        '''transpose转成上面的格式，批量求QK'''\n",
    "        scores = torch.bmm(query,key.transpose(1,2))/math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores,valid_len))\n",
    "        return torch.bmm(attention_weights,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten  = DotProductAttention(dropout=0.5)\n",
    "atten.eval()#关掉droput\n",
    "keys = torch.ones(2,10,2)\n",
    "#这里dim_v和dim_k好像可以不一样，一个是4，一个是2\n",
    "values = torch.arange(40,dtype=torch.float32).reshape(1,10,4).repeat(2,1,1)\n",
    "atten(torch.ones(2,1,2),keys,values,torch.tensor([2,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\alpha(k,q) = v^T \\tanh(W_k k + W_q q) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):\n",
    "    #key_size / query_size / units : k,q,v的embeddings size\n",
    "    def __init__(self,key_size,query_size,units,dropout,**kwargs):\n",
    "        super(MLPAttention,self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size,units,bias=False)\n",
    "        self.W_q = nn.Linear(query_size,units,bias=False)\n",
    "        self.v = nn.Linear(units,1,bias=False)#units既是value的维度，也是隐藏层的维度\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,query,key,value,valid_len):\n",
    "        query,key = self.W_q(query),self.W_k(key)\n",
    "        # Expand query to (`batch_size`, #queries, 1, units), and key to\n",
    "        # (`batch_size`, 1, #kv_pairs, units). Then plus them with broadcast\n",
    "        features = query.unsqueeze(2)+key.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        scores = self.v(features).squeeze(-1)\n",
    "    \n",
    "        attention_weights = self.dropout(masked_softmax(scores,valid_len))\n",
    "        \n",
    "        return torch.bmm(attention_weights,value)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = MLPAttention(2,2,8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPAttention(\n",
       "  (W_k): Linear(in_features=2, out_features=8, bias=False)\n",
       "  (W_q): Linear(in_features=2, out_features=8, bias=False)\n",
       "  (v): Linear(in_features=8, out_features=1, bias=False)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten(torch.ones(2,1,2),keys,values,torch.tensor([2,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An attention layer explicitly selects related information.\n",
    "注意力机制选择信息的关联强弱\n",
    "\n",
    "* An attention layer’s memory consists of key-value pairs, so its output is close to the values whose keys are similar to the queries.\n",
    "\n",
    "\n",
    "* Two commonly used attention models are dot product attention and MLP attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
