{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tflite.Interpreter(model_path=\"./model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'output_pvr/Sigmoid',\n",
       "  'index': 12,\n",
       "  'shape': array([1, 1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((1,4),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'inputs',\n",
       "  'index': 9,\n",
       "  'shape': array([1, 4], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01066639]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Interpreter in module tflite_runtime.interpreter:\n",
      "\n",
      "class Interpreter(builtins.object)\n",
      " |  Interpreter(model_path=None, model_content=None, experimental_delegates=None)\n",
      " |  \n",
      " |  Interpreter interface for TensorFlow Lite Models.\n",
      " |  \n",
      " |  This makes the TensorFlow Lite interpreter accessible in Python.\n",
      " |  It is possible to use this interpreter in a multithreaded Python environment,\n",
      " |  but you must be sure to call functions of a particular instance from only\n",
      " |  one thread at a time. So if you want to have 4 threads running different\n",
      " |  inferences simultaneously, create  an interpreter for each one as thread-local\n",
      " |  data. Similarly, if you are calling invoke() in one thread on a single\n",
      " |  interpreter but you want to use tensor() on another thread once it is done,\n",
      " |  you must use a synchronization primitive between the threads to ensure invoke\n",
      " |  has returned before calling tensor().\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, model_path=None, model_content=None, experimental_delegates=None)\n",
      " |      Constructor.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_path: Path to TF-Lite Flatbuffer file.\n",
      " |        model_content: Content of model.\n",
      " |        experimental_delegates: Experimental. Subject to change. List of\n",
      " |          [TfLiteDelegate](https://www.tensorflow.org/lite/performance/delegates)\n",
      " |          objects returned by lite.load_delegate().\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the interpreter was unable to create.\n",
      " |  \n",
      " |  allocate_tensors(self)\n",
      " |  \n",
      " |  get_input_details(self)\n",
      " |      Gets model input details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of input details.\n",
      " |  \n",
      " |  get_output_details(self)\n",
      " |      Gets model output details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of output details.\n",
      " |  \n",
      " |  get_tensor(self, tensor_index)\n",
      " |      Gets the value of the input tensor (get a copy).\n",
      " |      \n",
      " |      If you wish to avoid the copy, use `tensor()`. This function cannot be used\n",
      " |      to read intermediate results.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor_index: Tensor index of tensor to get. This value can be gotten from\n",
      " |                      the 'index' field in get_output_details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        a numpy array.\n",
      " |  \n",
      " |  get_tensor_details(self)\n",
      " |      Gets tensor details for every tensor with valid tensor details.\n",
      " |      \n",
      " |      Tensors where required information about the tensor is not found are not\n",
      " |      added to the list. This includes temporary tensors without a name.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of dictionaries containing tensor information.\n",
      " |  \n",
      " |  invoke(self)\n",
      " |      Invoke the interpreter.\n",
      " |      \n",
      " |      Be sure to set the input sizes, allocate tensors and fill values before\n",
      " |      calling this. Also, note that this function releases the GIL so heavy\n",
      " |      computation can be done in the background while the Python interpreter\n",
      " |      continues. No other function on this object should be called while the\n",
      " |      invoke() call has not finished.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When the underlying interpreter fails raise ValueError.\n",
      " |  \n",
      " |  reset_all_variables(self)\n",
      " |  \n",
      " |  resize_tensor_input(self, input_index, tensor_size)\n",
      " |      Resizes an input tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_index: Tensor index of input to set. This value can be gotten from\n",
      " |                     the 'index' field in get_input_details.\n",
      " |        tensor_size: The tensor_shape to resize the input to.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the interpreter could not resize the input tensor.\n",
      " |  \n",
      " |  set_tensor(self, tensor_index, value)\n",
      " |      Sets the value of the input tensor. Note this copies data in `value`.\n",
      " |      \n",
      " |      If you want to avoid copying, you can use the `tensor()` function to get a\n",
      " |      numpy buffer pointing to the input buffer in the tflite interpreter.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor_index: Tensor index of tensor to set. This value can be gotten from\n",
      " |                      the 'index' field in get_input_details.\n",
      " |        value: Value of tensor to set.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the interpreter could not set the tensor.\n",
      " |  \n",
      " |  tensor(self, tensor_index)\n",
      " |      Returns function that gives a numpy view of the current tensor buffer.\n",
      " |      \n",
      " |      This allows reading and writing to this tensors w/o copies. This more\n",
      " |      closely mirrors the C++ Interpreter class interface's tensor() member, hence\n",
      " |      the name. Be careful to not hold these output references through calls\n",
      " |      to `allocate_tensors()` and `invoke()`. This function cannot be used to read\n",
      " |      intermediate results.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```\n",
      " |      interpreter.allocate_tensors()\n",
      " |      input = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\n",
      " |      output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
      " |      for i in range(10):\n",
      " |        input().fill(3.)\n",
      " |        interpreter.invoke()\n",
      " |        print(\"inference %s\" % output())\n",
      " |      ```\n",
      " |      \n",
      " |      Notice how this function avoids making a numpy array directly. This is\n",
      " |      because it is important to not hold actual numpy views to the data longer\n",
      " |      than necessary. If you do, then the interpreter can no longer be invoked,\n",
      " |      because it is possible the interpreter would resize and invalidate the\n",
      " |      referenced tensors. The NumPy API doesn't allow any mutability of the\n",
      " |      the underlying buffers.\n",
      " |      \n",
      " |      WRONG:\n",
      " |      \n",
      " |      ```\n",
      " |      input = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])()\n",
      " |      output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
      " |      interpreter.allocate_tensors()  # This will throw RuntimeError\n",
      " |      for i in range(10):\n",
      " |        input.fill(3.)\n",
      " |        interpreter.invoke()  # this will throw RuntimeError since input,output\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor_index: Tensor index of tensor to get. This value can be gotten from\n",
      " |                      the 'index' field in get_output_details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A function that can return a new numpy array pointing to the internal\n",
      " |        TFLite tensor state at any point. It is safe to hold the function forever,\n",
      " |        but it is not safe to hold the numpy array forever.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tflite.Interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
